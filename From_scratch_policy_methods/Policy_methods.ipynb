{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "\n",
    "import gym, gym_walk, gym_aima\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "import random\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "random.seed(123); np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(pi, P, action_symbols=('<', 'v', '>', '^'), n_cols=4, title='Policy:'):\n",
    "    print(title)\n",
    "    arrs = {k:v for k,v in enumerate(action_symbols)}\n",
    "    for s in range(len(P)):\n",
    "        a = pi(s)\n",
    "        print(\"| \", end=\"\")\n",
    "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
    "            print(\"\".rjust(9), end=\" \")\n",
    "        else:\n",
    "            print(str(s).zfill(2), arrs[a].rjust(6), end=\" \")\n",
    "        if (s + 1) % n_cols == 0: print(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_success(env, pi, goal_state, n_episodes=100, max_steps=200):\n",
    "    random.seed(123); np.random.seed(123) ; env.seed(123)\n",
    "    results = []\n",
    "    \n",
    "    #run through 100 episodes\n",
    "    for _ in range(n_episodes):\n",
    "        state, done, steps = env.reset(), False, 0\n",
    "        \n",
    "        #if the exercise is done, or has reached the max steps end the episode\n",
    "        #until add the number of times that you made it to the goal state\n",
    "        while not done and steps < max_steps:\n",
    "            state, _, done, h = env.step(pi(state))\n",
    "            steps += 1\n",
    "        results.append(state == goal_state)\n",
    "        \n",
    "    #get a mean of the number of successful episodes\n",
    "    return np.sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_return(env, pi, n_episodes=100, max_steps=200):\n",
    "    random.seed(123); np.random.seed(123) ; env.seed(123)\n",
    "    results = []\n",
    "    \n",
    "    #run through 100 episodes\n",
    "    for _ in range(n_episodes):\n",
    "        state, done, steps = env.reset(), False, 0\n",
    "        results.append(0.0)\n",
    "        \n",
    "        #if the exercise is done, or has reached the max steps end the episode\n",
    "        #until then add the reward for each state\n",
    "        while not done and steps < max_steps:\n",
    "            state, reward, done, _ = env.step(pi(state))\n",
    "            results[-1] += reward\n",
    "            steps += 1\n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_value_function(V, P, n_cols=4, prec=3, title='State-value function:'):\n",
    "    print(title)\n",
    "    \n",
    "    for s in range(len(P)):\n",
    "        print(\"\\n\")\n",
    "        #print(P[s])\n",
    "    \n",
    "        v = V[s]\n",
    "        print(\"| \", end=\"\")\n",
    "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
    "            print(\"\".rjust(9), end=\" \")\n",
    "        else:\n",
    "            print(str(s).zfill(2), '{}'.format(np.round(v, prec)).rjust(6), end=\" \")\n",
    "        if (s + 1) % n_cols == 0: print(\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slippery Walk Five MDP and sample policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "|           | 01      < | 02      < | 03      < | 04      < | 05      < |           |\n",
      "Reaches goal 7.00%. Obtains an average undiscounted return of 0.0700.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SlipperyWalkFive-v0')\n",
    "P = env.env.P\n",
    "init_state = env.reset()\n",
    "goal_state = 6\n",
    "\n",
    "LEFT, RIGHT = range(2)\n",
    "pi = lambda s: {\n",
    "    0:LEFT, 1:LEFT, 2:LEFT, 3:LEFT, 4:LEFT, 5:LEFT, 6:LEFT\n",
    "}[s]\n",
    "print_policy(pi, P, action_symbols=('<', '>'), n_cols=7)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi, goal_state=goal_state)*100, \n",
    "    mean_return(env, pi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge v_{k+1}(s) = \\sum_{a} \\pi(a|s) \\sum_{a, s'} p(s',r|s, a)[r + \\gamma v_k(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(pi, P, gamma=1.0, theta=1e-10):\n",
    "    prev_V = np.zeros(len(P), dtype=np.float64) #initialize first iteration estimates\n",
    "    \n",
    "    while True:\n",
    "        V = np.zeros(len(P), dtype=np.float64)\n",
    "        \n",
    "        for s in range(len(P)):\n",
    "            count = 0\n",
    "            #calculate the value for all 7 (0 to 6) states\n",
    "            #prob for each state = 50% action success, 33.33% no effects, and 16.66% backward\n",
    "            #the not done is to make sure that terminal states are 0 reward.\n",
    "            for prob, next_state, reward, done in P[s][pi(s)]:\n",
    "                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))\n",
    "       \n",
    "        if np.max(np.abs(prev_V - V)) < theta:\n",
    "            break\n",
    "        prev_V = V.copy()\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-value function:\n",
      "\n",
      "\n",
      "|           \n",
      "\n",
      "| 01 0.00275 \n",
      "\n",
      "| 02 0.01099 \n",
      "\n",
      "| 03 0.03571 \n",
      "\n",
      "| 04 0.10989 \n",
      "\n",
      "| 05 0.33242 \n",
      "\n",
      "|           |\n"
     ]
    }
   ],
   "source": [
    "V = policy_evaluation(pi, P)\n",
    "print_state_value_function(V, P, n_cols=7, prec=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge \\pi'(s) = \\underset{a}{\\mathrm{argmin}} \\sum_{a, s'} p(s',r|s, a)[r + \\gamma v_\\pi(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V, P, gamma=1.0):\n",
    "    Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n",
    "    \n",
    "    #iterate through all 7 states (0 - 6)\n",
    "    for s in range(len(P)):\n",
    "        \n",
    "        # get the probs for left and right (0, 1)\n",
    "        for a in range(len(P[s])):\n",
    "            \n",
    "            # based on the policy we gather which is \n",
    "            #V = [0. 0.00274725 0.01098901 0.03571429 0.10989011 0.33241758 0.]\n",
    "            #which is the best direction, then we calculate for both directions\n",
    "            for prob, next_state, reward, done in P[s][a]:\n",
    "                Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "    \n",
    "    #          Left        Right\n",
    "    # Q = [[0.         0.        ]\n",
    "    #       [0.00274725 0.00641026]\n",
    "    #       [0.01098901 0.02197802]\n",
    "    #       [0.03571429 0.06868132]\n",
    "    #       [0.10989011 0.20879121]\n",
    "    #       [0.33241758 0.62912088]\n",
    "    #       [0.         0.        ]]\n",
    "\n",
    "    #get the index of the highest value in each row, this will give us the column that corresponds to left\n",
    "    #or right\n",
    "    new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    print(\"new policy\", [new_pi(i) for i in range(len(P))])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new policy [0, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "Policy:\n",
      "|           | 01      > | 02      > | 03      > | 04      > | 05      > |           |\n",
      "Reaches goal 93.00%. Obtains an average undiscounted return of 0.9300.\n"
     ]
    }
   ],
   "source": [
    "improved_pi = policy_improvement(V, P)\n",
    "print_policy(improved_pi, P, action_symbols=('<', '>'), n_cols=7)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, improved_pi, goal_state=goal_state)*100, mean_return(env, improved_pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-value function:\n",
      "\n",
      "\n",
      "|           \n",
      "\n",
      "| 01 0.66758 \n",
      "\n",
      "| 02 0.89011 \n",
      "\n",
      "| 03 0.96429 \n",
      "\n",
      "| 04 0.98901 \n",
      "\n",
      "| 05 0.99725 \n",
      "\n",
      "|           |\n"
     ]
    }
   ],
   "source": [
    "# how about we evaluate the improved policy?\n",
    "improved_V = policy_evaluation(improved_pi, P)\n",
    "print_state_value_function(improved_V, P, n_cols=7, prec=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new policy [0, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "Policy:\n",
      "|           | 01      > | 02      > | 03      > | 04      > | 05      > |           |\n",
      "Reaches goal 93.00%. Obtains an average undiscounted return of 0.9300.\n"
     ]
    }
   ],
   "source": [
    "# can we improved the improved policy?\n",
    "improved_improved_pi = policy_improvement(improved_V, P)\n",
    "print_policy(improved_improved_pi, P, action_symbols=('<', '>'), n_cols=7)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, improved_improved_pi, goal_state=goal_state)*100, \n",
    "    mean_return(env, improved_improved_pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-value function:\n",
      "\n",
      "\n",
      "|           \n",
      "\n",
      "| 01 0.66758 \n",
      "\n",
      "| 02 0.89011 \n",
      "\n",
      "| 03 0.96429 \n",
      "\n",
      "| 04 0.98901 \n",
      "\n",
      "| 05 0.99725 \n",
      "\n",
      "|           |\n"
     ]
    }
   ],
   "source": [
    "# it is the same policy\n",
    "# if we evaluate again, we can see there is nothing to improve \n",
    "# that also means we reached the optimal policy\n",
    "improved_improved_V = policy_evaluation(improved_improved_pi, P)\n",
    "print_state_value_function(improved_improved_V, P, n_cols=7, prec=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state-value function didn't improve, then we reach the optimal policy\n",
    "assert np.all(improved_V == improved_improved_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_reinforce",
   "language": "python",
   "name": "book_reinforce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
